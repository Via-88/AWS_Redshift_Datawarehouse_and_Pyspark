# Data Warehouse for Sparkify

## Introduction

Sparkify is a music streaming app. The analytics team is particularly interested in understanding what songs users are listening to. For this purpose, I built an ELT pipeline that extracts their data from S3, stages them in Redshift, and transforms data into a set of dimensional tables for their analytics team to continue finding insights in what songs their users are listening to.

## Database schema design and ETL pipeline

The database consists of 1 fact table and 4 dimensional tables. The fact table stores the relationship between an user and a song that the user listened at some time point. By using the four dimensional tables, we can reduce the duplicated information from fact table.

In this dataset, there are 7 tables created in this ETL pipeline.
* For staging_events table, I copy all the json data from `s3://dend/log_data` and use it to prepare all the dimension/fact tables.
* For staging_songs table, I copy all the json data from `s3://dend/song_data` and use it to prepare all the dimension/fact tables.
* For songs table, I select song_id, title, artist_id, year, duration from staging_songs table and insert them into this table.
* For artists table, I select artist_id, name, location, lattitude, longitude from staging_songs table and insert them into this table.
* For users table, I select each reach record from staging_events table, filter rows with `page="NextSong"` and save user_id, first_name, last_name, gender, level in this table.
* For time table, I select each reach record from staging_events table, filter rows with `page="NextSong"` and save start_time, hour, day, week, month, year, weekday in this table. Also, start_time will be converted from ts. Except start_time, other information will be generated by start_time.
* For songplay_table, I first inner join both songs and artists tables, left join this table with staging_events, and save start_time, userId, level, song_id, artist_id, sessionId, location, userAgent.

